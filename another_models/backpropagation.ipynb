{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Una red neuronal es una función $fw(x)$ de $x$ que depende de unos parámetros $\\omega$. $x$ serán los datos que queramos predecir y $\\omega$ son unos parámetros que nos permiten ajustar la red a los datos. Para poder encontrar una red neuronal que se ajuste bien a los datos, necesitamos calcular $\\partial w f_w(x)$ (observad que la derivada parcial es respecto a $w$ y no respecto a $x$). El algoritmo que se nutre de las derivadas parciales para ajustar los pesos se denomina backpropagation.\n",
    "\n",
    "La red de la Figura 2 es una versi ́on simplificada de lo que se llama red neuronal recurrente.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./img/graph_propagation.png\" alt=\"Figura 2\">\n",
    "</p>\n",
    "\n",
    "## Analogía de Backpropagation: El Viaje del Tren\n",
    "\n",
    "Imagina que tienes un tren de juguete que pasa por varias estaciones antes de llegar a su destino. Cada estación afecta de alguna manera al tren, y quieres saber cómo una estación específica impacta en el tren al llegar a su destino.\n",
    "\n",
    "En nuestro caso, el tren es \\(y\\), y las estaciones son \\(s_2\\), \\(z_2\\), \\(s_1\\), y \\(z_1\\). Queremos saber cómo una pequeña variación en \\(x_1\\) (una de las estaciones) afecta al destino final, \\(y\\).\n",
    "\n",
    "Para ello, utilizamos algo llamado \"regla de la cadena\", que es como si tomáramos una lupa y observáramos cómo cada estación afecta al tren, una por una, hasta llegar a su destino.\n",
    "\n",
    "**Construcción de la Cadena**\n",
    "\n",
    "¿Cómo construimos la \"cadena\"?\n",
    "\n",
    "1. \\( \\frac{\\partial y}{\\partial s_2} \\): Miramos cómo afecta la estación \\(s_2\\) al destino final \\(y\\).\n",
    "2. \\( \\frac{\\partial s_2}{\\partial z_2} \\): Después, miramos cómo la estación \\(z_2\\) afecta a la siguiente estación, \\(s_2\\).\n",
    "3. \\( \\frac{\\partial z_2}{\\partial s_1} \\): Continuamos observando. Ahora, cómo la estación \\(s_1\\) afecta a \\(z_2\\).\n",
    "4. \\( \\frac{\\partial s_1}{\\partial z_1} \\): Luego, cómo \\(z_1\\) afecta a \\(s_1\\).\n",
    "5. \\( \\frac{\\partial z_1}{\\partial x_1} \\): Finalmente, esta es la estación que realmente nos interesa. Miramos cómo una pequeña variación en \\(x_1\\) afecta a \\(z_1\\).\n",
    "\n",
    "Multiplicando todos estos efectos juntos, obtienes cómo \\(x_1\\) afecta finalmente a \\(y\\), pasando por todas las estaciones intermedias.\n",
    "\n",
    "La segunda parte de la cadena, es un camino más corto, es como si el tren tuviera una ruta alternativa para llegar a su destino. Es solo mirar cómo afecta la estación \\(s_1\\) a \\(y\\), y luego cómo \\(z_1\\) afecta a \\(s_1\\), y finalmente cómo \\(x_1\\) afecta a \\(z_1\\).\n",
    "\n",
    "Al sumar los efectos de ambas rutas, obtienes la influencia total de \\(x_1\\) en \\(y\\).\n",
    "\n",
    "\n",
    "\n",
    "La razón por la que tenemos dos conjuntos en la cadena se debe a cómo está estructurada la función final $y$.  Si observas la definición: $y=s_1+s_2$ , Así que, volviendo a nuestra analogía del tren, es como si tuviéramos dos caminos para llegar a $y$.\n",
    "\n",
    "\n",
    "## ejemplo del problema teórico\n",
    "\n",
    "\n",
    "Dada la función de activación $h(t)=1 / {1+e^{-t'}}$ y las funciones \n",
    "\n",
    "* $z_1 = \\omega x_1 + r x_2$   ;   \n",
    "* $s_1 = h(z_1)$     ;     \n",
    "* $z_2 = s_1 + \\theta x_2 + \\omega x_3 + \\delta x_4$   ;   \n",
    "* $s_2 = h(z_2)$    ;    \n",
    "* $y = s_2. + s_1$    ;  \n",
    "\n",
    "\n",
    "calcula las derivadas parciales  $\\frac{\\partial y}{\\partial x_1}$ y $\\frac{\\partial y}{\\partial x_2}$\n",
    "\n",
    "**interpretación de los que vamos hacer:**\n",
    "\n",
    "Dado que vamos a calcular las derivadas parciales $\\frac{\\partial y}{\\partial x_1}$ y $\\frac{\\partial y}{\\partial x_2}$, procedamos a interpretar los resultados en el contexto de la red neuronal.\n",
    "\n",
    "La derivada $\\frac{\\partial y}{\\partial x_2}$ representa cuánto cambia la salida $y$ de la red con respecto a un pequeño cambio en $x_1$. Esto es crucial para ajustar los pesos $w$ en la red neuronal durante el entrenamiento porque nos indica la dirección y la magnitud en la que **se deben ajustar** los parámetros para minimizar el error de predicción.\n",
    "\n",
    "La misma interpretaciçon para $\\frac{\\partial y}{\\partial x_2}$\n",
    "\n",
    "En el contexto de backpropagation, estas derivadas parciales nos permiten calcular los gradientes para cada parámetro de la red, lo cual guía el ajuste de dichos parámetros. Estos gradientes, cuando son multiplicados por la tasa de aprendizaje, indican cuánto y en qué dirección ajustar cada parámetro para mejorar el rendimiento de la red.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./img/graph_propagation.png\" alt=\"Figura 2\">\n",
    "</p>\n",
    "\n",
    "Resumen de los resultados:\n",
    "\n",
    "Para $x_1$: \n",
    "\n",
    "$\\frac{\\partial y}{\\partial x_1} = \\omega s_1 (1 - s_1)$ \n",
    "\n",
    "Para $x_2$:\n",
    "\n",
    "$\\frac{\\partial y}{\\partial x_2} = \\theta s_2 (1 - s_2) + rs_1 (1 - s_1)$\n",
    "\n",
    "Si en la etapa de entrenamiento observamos que, por ejemplo, el error de predicción es positivo, querríamos ajustar los parámetros de la red en la dirección negativa del gradiente para minimizar ese error. Estos cálculos son esenciales en cada paso del entrenamiento para ajustar la red y lograr que las predicciones sean lo más precisas posible.\n",
    "\n",
    "### Desarrollo de \\(\\frac{\\partial y}{\\partial x_1}\\)\n",
    "\n",
    "Dada la función \\(y = s_2 + s_1\\),  \n",
    "usamos la regla de la cadena para descomponer la derivada:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial y}{\\partial x_1} = \n",
    "\\frac{\\partial y}{\\partial s_1} \n",
    "\\frac{\\partial s_1}{\\partial z_1} \n",
    "\\frac{\\partial z_1}{\\partial x_1}\n",
    "\\]\n",
    "\n",
    "**a) Derivada de \\(y\\) respecto a \\(s_1\\)**\n",
    "\n",
    "En \\(y = s_2 + s_1\\) el cambio que puede tener $y$ cuando se cambia $s_1$ (mantiendo $s_1$ constante), es exactamanete el mismo valor que el cambio de $s_1$. Esto es equivalente a decir que la derivada de $y$ respecto $s_1$ es 1. El término $S_2$ se convierte en cero porque estamos tomando la dereivada respecto a $s_1$ entonces  $S_2$ se trata como una constante y la derivada de una constante es cero.\n",
    "\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial y}{\\partial s_1} = 1\n",
    "\\]\n",
    "\n",
    "\n",
    "**b) Derivada de \\(s_1\\) respecto a \\(z_1\\)**\n",
    "\n",
    "según enunciado  $s_1 = h(z_1)$\n",
    "\n",
    "Usando la función de activación \\(h(t) = \\frac{1}{{1+e^{-t}}}\\):\n",
    "\n",
    "$s_1 = h(z_1) = \\frac{1}{{1+e^{-z_1}}}$\n",
    "\n",
    "Para calcular la derivada de $h(t)$ respecto a t, que es $h´(t)$ tenemos una propiedad común de la función sigmoide. \n",
    "\n",
    "$h'(t) = h(t) \\cdot (1 - h(t))$.  \n",
    "\n",
    "$h'(z_1) = h(z_1) \\cdot (1 - h(z_1))$.   \n",
    "\n",
    "$h'(z_1) = s_1 \\cdot (1 - s_1)$.   \n",
    "\n",
    "Entonces\n",
    "\n",
    "$$\\frac{\\partial s_1}{\\partial z_1} = s_1 \\cdot (1 - s_1)$$ \n",
    "\n",
    "Esta derivada de \\(s_1\\) respecto a \\(z_1\\) representa como un cambio en $z_1$ afecta a $s_1$ teniendo en cuenta la función de activación sigmoide.\n",
    "\n",
    "\n",
    "**c) Derivada de \\(z_1\\) respecto a \\(x_1\\)**\n",
    "\n",
    "Dado que derivamos en funciín de $x_1$ entonces tratamos $r x_2$ como constante y la derivada de una constante es cero. \n",
    "\n",
    "$z_1 = \\omega x_1 + r x_2$\n",
    "\n",
    "Entonces la derivada de $z_1$ respecto $x_1$ es\n",
    "\n",
    "\\[\\frac{\\partial z_1}{\\partial x_1} = \\omega\\]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Juntando todas las derivadas parciales, obtenemos:**\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_1} = 1 \\cdot (s_1 (1-s_1) \\cdot \\omega = $$\n",
    "\n",
    "$$= \\omega s_1 \\cdot (1-s_1)$$\n",
    "\n",
    "  \n",
    "$$\\text{Esa es la derivada de $y$ con respecto a $x_1$}$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Desarrollo de \\(\\frac{\\partial y}{\\partial x_2}\\)\n",
    "\n",
    "Dada la función \\(y = s_2 + s_1\\), descomponemos la derivada usando la regla de la cadena:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial y}{\\partial x_2} = \n",
    "\\frac{\\partial y}{\\partial s_2} \n",
    "\\frac{\\partial s_2}{\\partial z_2} \n",
    "\\frac{\\partial z_2}{\\partial x_2} + \n",
    "\\frac{\\partial y}{\\partial s_1} \n",
    "\\frac{\\partial s_1}{\\partial z_1} \n",
    "\\frac{\\partial z_1}{\\partial x_2}\n",
    "\\]\n",
    "\n",
    "**a) Derivada de \\(y\\) respecto a \\(s_2\\) y derivada de \\(y\\) respecto a \\(s_1\\)**\n",
    "\n",
    "$\\frac{\\partial y}{\\partial s_2} = 1$ (porque $y = s_2 + s_1$)\n",
    "\n",
    "$\\frac{\\partial y}{\\partial s_1} = 1$ (porque $y = s_2 + s_1$)\n",
    "\n",
    "**b) Derivada de \\(s_2\\) respecto a \\(z_2\\)**\n",
    "\n",
    "Usando la función de activación \\(h(t) = \\frac{1}{{1+e^{-t}}}\\), tenemos:\n",
    "\n",
    "\n",
    "$\\frac{\\partial s_2}{\\partial z_2} = \\frac{\\partial h(z_2)}{\\partial z_2} = h(z_2) \\cdot (1 - h(z_2)) = s_2 \\cdot (1 - s_2)$\n",
    "\n",
    "\n",
    "**c) Derivada de \\(z_2\\) respecto a \\(x_2\\)**\n",
    "\n",
    "$z_2 = s_1 + \\theta x_2 + \\omega x_3 + \\delta x_4$\n",
    "\n",
    "$\\frac{\\partial z_2}{\\partial x_2} = \\theta$\n",
    "\n",
    "**d) Derivada de \\(s_1\\) respecto a \\(z_1\\)**\n",
    "\n",
    "$\\frac{\\partial s_1}{\\partial z_1} = h(z_1) \\cdot (1 - h(z_1)) = s_1 \\cdot (1 - s_1)$\n",
    "\n",
    "**f) Derivada de \\(z_1\\) respecto a \\(x_2\\)**\n",
    "\n",
    "$\\frac{\\partial z_1}{\\partial x_2} = r$\n",
    "\n",
    "Uniendo todas las derivadas parciales, obtenemos:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_2} = 1 \\times s_2 \\cdot (1 - s_2) \\times \\theta + 1 \\times s_1 \\cdot (1 - s_1) \\times r =$$\n",
    "\n",
    "$$= \\theta s_2 \\cdot (1 - s_2) + r s_1 \\cdot (1 - s_1)$$\n",
    "\n",
    "$$\\text{Esa es la derivada de $y$ con respecto a $x_2$}$$\n",
    "\n",
    "## Analogía de Backpropagation: La Cotización de una Compañía\n",
    "\n",
    "Imagina que estás tratando de determinar la cotización de una compañía en el mercado de valores. Esta cotización, que llamaremos **Precio Final de Cotización (PFC)**, no se basa simplemente en un solo factor, sino en múltiples factores y atributos financieros que determinan la salud y la proyección de una compañía.\n",
    "\n",
    "**¿Cómo se relaciona esto con backpropagation?**\n",
    "\n",
    "Al igual que una red neuronal tiene diferentes capas y nodos que influyen en el resultado final, el PFC de una compañía es influenciado por una serie de factores financieros, desde los más obvios hasta los más ocultos.\n",
    "\n",
    "**Las \"Capas\" del PFC**\n",
    "\n",
    "1. **Datos Financieros**: Son los estados financieros básicos de la compañía. Incluyen la Cuenta de Resultados, el Balance y el Flujo de Caja. Estos son como la \"primera capa\" en una red neuronal.\n",
    "\n",
    "2. **Multiplicadores y Factores**: A partir de los datos financieros, podemos derivar una serie de multiplicadores y factores, como los múltiplos y factores que mencionaste (Forward Multiples, Trailing Multiples, etc.). Estos actúan como la \"capa intermedia\" en nuestra red.\n",
    "\n",
    "3. **Estimaciones**: Finalmente, basándonos en todo lo anterior, podemos hacer ciertas estimaciones, como los ingresos futuros o márgenes de EBITDA. Esta es la \"última capa\" antes de llegar a nuestro PFC.\n",
    "\n",
    "**Aplicando Backpropagation**\n",
    "\n",
    "Imagina que observas una discrepancia en el PFC actual de la compañía en comparación con lo que crees que debería ser. Quieres ajustar tus modelos y factores para que se alinee mejor con la realidad. Pero, ¿cómo sabes qué atributos financieros debes ajustar? Aquí es donde entra el algoritmo de backpropagation.\n",
    "\n",
    "Al igual que backpropagation en redes neuronales ajusta los pesos para minimizar el error, en este contexto, ajustaríamos y recalibraríamos los distintos factores y multiplicadores financieros, \"retrocediendo\" desde el PFC hasta llegar a los datos financieros brutos.\n",
    "\n",
    "**¿Por qué es útil?**\n",
    "\n",
    "Backpropagation, en este contexto, nos permite entender cuánto impacto tiene un cambio en, por ejemplo, un elemento del Balance o de la Cuenta de Resultados en el PFC. Así, si hay un cambio significativo en un informe trimestral, puedes estimar cómo esto podría afectar el PFC. Es una herramienta valiosa para ajustar y refinar nuestros modelos financieros, asegurando que estén lo más alineados posible con la realidad del mercado.\n",
    "\n",
    "---\n",
    "\n",
    "En el mundo financiero, nos permite ajustar y refinar nuestras evaluaciones y modelos, asegurando que reflejen de la mejor manera posible la realidad compleja y multifacética del mundo financiero.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
