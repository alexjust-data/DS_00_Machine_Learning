{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1goSZkfi2rz4"
      },
      "source": [
        "<div style=\"width: 100%; clear: both;\">\n",
        "<div style=\"float: left; width: 50%;\">\n",
        "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
        "</div>\n",
        "<div style=\"float: right; width: 50%;\">\n",
        "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">22.418 · Aprenentatge automàtic</p>\n",
        "<p style=\"margin: 0; text-align:right;\">Grau en Ciència de Dades Aplicada</p>\n",
        "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudis de Informàtica, Multimèdia i Telecomunicació</p>\n",
        "</div>\n",
        "</div>\n",
        "<div style=\"width:100%;\">&nbsp;</div>\n",
        "\n",
        "\n",
        "# Module 2: Naive Bayes example\n",
        "\n",
        "In this notebook we will implement a Naive Bayes classifier to distinguish between the different classes existent in two different datasets: the [Mushroom data set](https://archive.ics.uci.edu/ml/datasets/Mushroom) and the <a href=\"https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\">Iris flower data set</a>.\n",
        "\n",
        "We will start with a binary dataset (i.e. the Mushroom data set) and then move to a multi-class data set and learn how scikit-learn is able to automatically deal with multi-class for us.\n",
        "\n",
        "The Mushroom dataset contains data on which mushrooms are edible and which are poisonous. There are 8124 mushrooms defined in the dataset of which 4208 are edible and 3915 are poisonous. Each of these is characterized by 22 features.\n",
        "\n",
        "On the other hand, the Iris flower dataset consists of 3 different types of irises' (Setosa, Versicolor, and Virginica) petal and sepal length, stored in a 150x4 numpy ndarray. The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juQo1ZyD3MOz"
      },
      "source": [
        "First of all, let's see the libraries we will use during our practical assignment:\n",
        "\n",
        "### [numpy](https://numpy.org)\n",
        "\n",
        "The NumPy library for Python concentrates on handling extensive multi-dimensional data and the intricate mathematical functions operating on the data. NumPy offers speedy computation and execution of complicated functions working on arrays. Few of the points in favor of NumPy are:\n",
        "\n",
        "- Support for mathematical and logical operations\n",
        "- Shape manipulation\n",
        "- Sorting and Selecting capabilities\n",
        "- Discrete Fourier transformations\n",
        "- Basic linear algebra and statistical operations\n",
        "- Random simulations\n",
        "- Support for n-dimensional arrays\n",
        "\n",
        "NumPy works on an object-oriented approach and has tools for integrating C, C++ and Fortran code, and this makes NumPy highly popular amongst the scientific community.\n",
        "\n",
        "### [pandas](https://pandas.pydata.org)\n",
        "\n",
        "Pandas is a Python data analysis library and is used primarily for data manipulation and analysis. It comes into play before the dataset is prepared for training. Pandas make working with time series and structured multidimensional data effortless for machine-learning programmers. Some of the great features of Pandas when it comes to handling data are:\n",
        "\n",
        "- Dataset reshaping and pivoting\n",
        "- Merging and joining of datasets\n",
        "- Handling of missing data and data alignment\n",
        "- Various indexing options such as Hierarchical axis indexing, Fancy indexing\n",
        "- Data filtration options\n",
        "\n",
        "Pandas makes use of DataFrames, which is just a technical term for a two-dimensional representation of data by offering programmers with DataFrame objects.\n",
        "\n",
        "### [scikit-learn](https://scikit-learn.org)\n",
        "\n",
        "Scikit-learn is a very actively used machine learning library for Python. It includes easy integration with different ML programming libraries like NumPy and Pandas. Scikit-learn comes with the support of various algorithms such as:\n",
        "\n",
        "- Classification\n",
        "- Regression\n",
        "- Clustering\n",
        "- Dimensionality Reduction\n",
        "- Model Selection\n",
        "- Preprocessing\n",
        "\n",
        "Built around the idea of being easy to use but still be flexible, Scikit-learn is focussed on data modelling and not on other tasks such as loading, handling, manipulation and visualization of data. It is considered sufficient enough to be used as an end-to-end ML, from the research phase to the deployment.\n",
        "\n",
        "### [matplotlib](https://matplotlib.org)\n",
        "\n",
        "Simply put, matplotlib is a plotting library. If you’ve ever used MATLAB before, you’ll probably feel very comfortable in the matplotlib environment. When analyzing images, we’ll make use of matplotlib, whether plotting the overall accuracy of search systems or simply viewing the image itself, matplotlib is a great tool to have in your toolbox.\n",
        "\n",
        "Matplotlib is a comprehensive data visualization library in Python. Some features include:\n",
        "\n",
        "- creating interactive plots.\n",
        "- offering flexible customization of the plot.\n",
        "\n",
        "It is a foundation library supporting seaborn, which is easier to use. But when we want to customize the plots more, matplotlib becomes necessary. It's also useful for quick tests and visualizations.\n",
        "\n",
        "### [seaborn](https://seaborn.pydata.org)\n",
        "\n",
        "Seaborn is a popular Python library for making statistical data visualizations. It is based on matplotlib and also integrated with pandas data structures.\n",
        "\n",
        "Seaborn is especially useful for exploring and understanding data. Some of the functionalities that seaborn offers:\n",
        "\n",
        "- options for visualizing univariate and bivariate distributions.\n",
        "- options for visualizing numerical and categorical variables.\n",
        "- automatic estimation and plotting of linear regression models.\n",
        "- ability to build complex visualizations such as multi-plot grids.\n",
        "\n",
        "It comes in handy whenever whe need to explore a dataset with Histograms, Heatmaps, Scatter plots, Barplots, etc.\n",
        "\n",
        "\n",
        "This is a general introduction of each of the libraries we will use. For more information, feel free to refer to their official webpages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmJwuySkYcOc"
      },
      "source": [
        "### Mushrooms data set\n",
        "\n",
        "Now, as we said, we will start with the Mushroom dataset. To do so, we will first import everything we need:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4_cH0N82L3F"
      },
      "source": [
        "# Import packages\n",
        "from sklearn import datasets\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziD3J8cmYph1"
      },
      "source": [
        "After loading the libraries, we need to load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_c2wOfPYsia"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
        "!cat agaricus-lepiota.data | head "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE729EuJbTQc"
      },
      "source": [
        "The first column shows the class: \"p\" for poisonous, \"e\" for edible. The rest of columns are the features. Let's load the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3Zlkm4vcB3Z"
      },
      "source": [
        "mushrooms = pd.read_csv('agaricus-lepiota.data')\n",
        "mushrooms.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide between the feature variables and the class: "
      ],
      "metadata": {
        "id": "cCQRuc7C7MAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mushrooms_X = mushrooms.drop(['p'], axis=1)\n",
        "y = mushrooms.values[:, 0]"
      ],
      "metadata": {
        "id": "iQfg2wMe6y8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKaeem9wpGQ8"
      },
      "source": [
        "All the variables are categorical, so we need to convert them to numerical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcpG9piToy9C"
      },
      "source": [
        "# All the variables are in string format. Convert categorical variables to integer using pandas get_dummies function\n",
        "\n",
        "for col in mushrooms_X.columns:\n",
        "  mushrooms_X = pd.get_dummies(mushrooms_X,prefix=[col], columns = [col], drop_first=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-ckReojpEJY"
      },
      "source": [
        "mushrooms_X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjxEMGDtpbuT"
      },
      "source": [
        "Now that we have the dataset all with numerical values, we can separate the data and the labels and split in training and testing set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7n6e28zZevV"
      },
      "source": [
        "As you all already know, in order to carry out experiments, we need to split the dataset in two subsets, one for training and another one for testing. We'll do it using scikit-learn `train_test_split` function, as we learnt in the first unit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3m7VEe8-IjY"
      },
      "source": [
        "x = mushrooms_X.values[:, 1:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8K381MmZevs"
      },
      "source": [
        "# Split train and test\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap-sm9BiZevt"
      },
      "source": [
        "Let's check the shape of each subset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVI1vnFJZevt"
      },
      "source": [
        "print('xtrain shape: ', xtrain.shape)\n",
        "print('ytrain shape: ', ytrain.shape)\n",
        "print('xtest shape: ', xtest.shape)\n",
        "print('ytest shape: ', ytest.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxYbhHVRZevv"
      },
      "source": [
        "At this point we have everything we need to start implementing our classifier. In this example we will use the Naive Bayes rule to classify the different samples in our dataset.\n",
        "\n",
        "`scikit-learn` implements such a classifier in its `GaussianNB` class. It only has two parameters:\n",
        "\n",
        "```\n",
        "priors: array-like of shape (n_classes,)\n",
        "Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n",
        "\n",
        "var_smoothing: float, default=1e-9\n",
        "Portion of the largest variance of all features that is added to variances for calculation stability.\n",
        "```\n",
        "\n",
        "For our task, we'll leave them by default.\n",
        "\n",
        "Let's create the classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk7kPynhZevw"
      },
      "source": [
        "# Init the Gaussian Classifier\n",
        "model = GaussianNB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPRDZbhqZevw"
      },
      "source": [
        "Now, we can use the `train` method to fit the classifier to our train set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSnsexKyZevw"
      },
      "source": [
        "# Train the model \n",
        "model.fit(xtrain, ytrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-iyzyyGZevx"
      },
      "source": [
        "And once it's trained, use the model to predict the samples in the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_iF1bvTZevy"
      },
      "source": [
        "# Predict Output \n",
        "preds = model.predict(xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPtMK5UEZevy"
      },
      "source": [
        "Let's see the result. We'll compute the confusion matrix, which contains information about true positives/negatives and false positives/negatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5lls_giZevy"
      },
      "source": [
        "# Plot Confusion Matrix\n",
        "mat = confusion_matrix(ytest, preds)\n",
        "names = np.unique(preds)\n",
        "sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=names, yticklabels=names)\n",
        "plt.xlabel('Truth')\n",
        "plt.ylabel('Predicted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiUB4D2sZevz"
      },
      "source": [
        "We can also compute the most common classification metrics with the help of `sklearn.metrics.classification_report`. These metrics include the precission, recall, f1-score and support of each class. For more information regarding these metrics, you can refer to the theoretical documents of the course or access the <a hred=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\">documentation</a>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLEGL9_CZev0"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(ytest, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A885AHvoZev1"
      },
      "source": [
        "Furthermore, thanks to the probabilistic behaviour of the Naive Bayes classifier, we can easily access the probability of each instance belonging to each lass:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoEDKqq3Zev1"
      },
      "source": [
        "preds_proba = model.predict_proba(xtest)\n",
        "print(preds_proba[:10, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAYKet6wZev2"
      },
      "source": [
        "These probabilities show the probability of each instance to belong to each of the two classes: edible and poisonous."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZcS6R903Pms"
      },
      "source": [
        "### Iris flower dataset\n",
        "\n",
        "For the Iris flower dataset, we'll use scikit-learn's `datasets` class, which gives easy access to common toy datasets used in Machine Learning/Deep Learning.\n",
        "\n",
        "In this example, we will deal with a multiclass problem automatically thanks to the internal mechanisms of `scikit-learn`, that will deal with it for us. From now on, we will carry out all the examples with the Iris flower dataset.\n",
        "\n",
        "The data itself is contained in `iris.data`, and the labels are in `iris.target`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu65HdP13XzC"
      },
      "source": [
        "# Import data\n",
        "iris = datasets.load_iris()\n",
        "x = iris.data\n",
        "y = iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mumQr0P45nY"
      },
      "source": [
        "As you all already know, in order to carry out experiments, we need to split the dataset in two subsets, one for training and another one for testing. We'll do it using scikit-learn `train_test_split` function, as we learnt in the first unit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNoEI_jY4gF7"
      },
      "source": [
        "# Split train and test\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bFnKuG848Mx"
      },
      "source": [
        "Let's check the shape of each subset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxCYp1lC4-3h"
      },
      "source": [
        "print('xtrain shape: ', xtrain.shape)\n",
        "print('ytrain shape: ', xtrain.shape)\n",
        "print('xtest shape: ', xtest.shape)\n",
        "print('ytest shape: ', xtest.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gly7LTgt3ygg"
      },
      "source": [
        "At this point we have everything we need to start implementing our classifier. In this example we will use the Naive Bayes rule to classify the different samples in our dataset.\n",
        "\n",
        "`scikit-learn` implements such a classifier in its `GaussianNB` class. It only has two parameters:\n",
        "\n",
        "```\n",
        "priors: array-like of shape (n_classes,)\n",
        "Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n",
        "\n",
        "var_smoothing: float, default=1e-9\n",
        "Portion of the largest variance of all features that is added to variances for calculation stability.\n",
        "```\n",
        "\n",
        "For our task, we'll leave them by default.\n",
        "\n",
        "Let's create the classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-_7Vj-72QLo"
      },
      "source": [
        "# Init the Gaussian Classifier\n",
        "model = GaussianNB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osbfANSe38sk"
      },
      "source": [
        "Now, we can use the `train` method to fit the classifier to our train set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hc3iVOE2SqJ"
      },
      "source": [
        "# Train the model \n",
        "model.fit(xtrain, ytrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJhO3uxA3_XT"
      },
      "source": [
        "And once it's trained, use the model to predict the samples in the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AnMPKRr2U44"
      },
      "source": [
        "# Predict Output \n",
        "preds = model.predict(xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKcNdfcp4LKC"
      },
      "source": [
        "Let's see the result. We'll compute the confusion matrix, which contains information about true positives/negatives and false positives/negatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZyisM4Z2XdR"
      },
      "source": [
        "# Plot Confusion Matrix\n",
        "mat = confusion_matrix(preds, ytest)\n",
        "names = np.unique(preds)\n",
        "sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=names, yticklabels=names)\n",
        "plt.xlabel('Truth')\n",
        "plt.ylabel('Predicted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A40CKl5KKa-"
      },
      "source": [
        "As you can see, the model behaves almost perfectly, with only 2 miss-classifications.\n",
        "\n",
        "We can also compute the most common classification metrics with the help of `sklearn.metrics.classification_report`. These metrics include the precission, recall, f1-score and support of each class. For more information regarding these metrics, you can refer to the theoretical documents of the course or access the <a hred=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\">documentation</a>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPuEAGQbK1XY"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(ytest, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIwk4Bpr8P_0"
      },
      "source": [
        "Furthermore, thanks to the probabilistic behaviour of the Naive Bayes classifier, we can easily access the probability of each instance belonging to each lass:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCO4zqW78VKW"
      },
      "source": [
        "preds_proba = model.predict_proba(xtest)\n",
        "print(preds_proba[:10, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-vwcjtcx0gS"
      },
      "source": [
        "These probabilities show the probability of each instance to belong to each of the three classes: setosa, versicolor and virginica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-RN95cmO7Kg"
      },
      "source": [
        "Finally, we can also graphically show the decision boundaries for the 3 classes. However, since we will visualize them in 2 dimensions, we'll need to re-train the algorithm with just 2 of the 4 available variables. For this example, we'll take the first 2, but feel free to try with any other combination!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQHUX7EFO_f7"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "X = iris.data[:, :2] # to visualize in 2-dim, we can only take 2 variables\n",
        "h = .02  # step size in the mesh\n",
        "\n",
        "# Create color maps\n",
        "cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
        "cmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])\n",
        "\n",
        "# Plot the decision boundary. For that, we will assign a color to each\n",
        "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "# Train the model \n",
        "model.fit(xtrain[:, :2], ytrain) # to visualize in 2-dim, we can only take 2 variables\n",
        "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Put the result into a color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.figure()\n",
        "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "# Plot also the training points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n",
        "plt.title(\"3-Class classification\")\n",
        "plt.axis('tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sfy7voK1x-fp"
      },
      "source": [
        "Another possible option would be to apply a PCA to the Iris data to reduce the number of dimensions to just 2 and then use the transformed data to visualise the decision boundaries. You are welcome to try it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uw7nvvv0dS_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}
